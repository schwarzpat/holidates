Confluence uses a different syntax for rendering math equations since it does not natively support LaTeX. To include the formulas in Confluence, you can use the MathJax or LaTeX Math Macro plugins, if available. Below is how the Weighted Quantile Loss (WQL) formulas would look for Confluence-compatible environments:

Quantile Loss (QL) Formula in Confluence

If MathJax or similar plugins are supported, use the following:

\[
QL_q(y, \hat{y}) =
\begin{cases} 
q \cdot (y - \hat{y}) & \text{if } y \geq \hat{y} \\ 
(1 - q) \cdot (\hat{y} - y) & \text{if } y < \hat{y}
\end{cases}
\]

For context, write:
	•	y is the observed value.
	•	\hat{y} is the predicted quantile.

Weighted Quantile Loss (WQL) Formula in Confluence

\[
WQL = \sum_{q \in Q} w_q \cdot QL_q(y, \hat{y}_q)
\]

For explanation:
	•	Q is the set of quantiles (e.g., {0.1, 0.5, 0.9}).
	•	w_q is the weight assigned to quantile q.
	•	\hat{y}_q is the predicted value for quantile q.

Without Math Plugins

If Confluence does not support math rendering plugins, you can write the formulas in plain text:

Quantile Loss (QL):

QL_q(y, ŷ) =
    q * (y - ŷ), if y >= ŷ
    (1 - q) * (ŷ - y), if y < ŷ

Weighted Quantile Loss (WQL):

WQL = Σ (w_q * QL_q(y, ŷ_q)), for all q ∈ Q

You can add the explanation as normal text:
	•	y is the observed value.
	•	ŷ is the predicted quantile value.
	•	w_q is the weight for each quantile q.

Using plain text works across all Confluence setups but lacks visual elegance. If plugins are available, the LaTeX version is preferred for clarity.


For a given quantile `q ∈ (0, 1)`, the quantile loss is defined as:

:<math>
QL_q(y, \hat{y}) =
\begin{cases} 
q \cdot (y - \hat{y}) & \text{if } y \geq \hat{y} \\ 
(1 - q) \cdot (\hat{y} - y) & \text{if } y < \hat{y} 
\end{cases}
</math>

Where:
* `y` is the true observed value
* `\hat{y}` is the predicted quantile value

The Weighted Quantile Loss (WQL) generalizes QL by assigning a weight `w_q` to each quantile `q`:

:<math>
WQL = \sum_{q \in Q} w_q \cdot QL_q(y, \hat{y}_q)
</math>

Where:
* `Q` is the set of quantiles being predicted (e.g., `Q = {0.1, 0.5, 0.9}`)
* `w_q` is the weight assigned to quantile `q`
* `\hat{y}_q` is the predicted value for quantile `q`



AutoGluon is an open-source automated machine learning (AutoML) framework that aims to simplify the model selection, hyperparameter optimization, and ensemble creation for predictive tasks. Below, I will explain the specific models listed in your data.
Time series forecasting involves predicting future values based on previously observed data points collected at regular intervals. AutoGluon facilitates this by providing the TimeSeriesPredictor, which can handle multiple related time series simultaneously. This predictor can generate forecasts that include both the mean (expected) values and the quantiles of the forecast distribution, offering insights into the range of possible future outcomes.
Models Listed and Their Functionality in AutoGluon

AutoGluon integrates a diverse set of forecasting models to enhance prediction accuracy. These include:
	•	Statistical Models: Traditional methods like Exponential Smoothing State Space Model (ETS) and AutoRegressive Integrated Moving Average (ARIMA) are utilized for their effectiveness in capturing patterns in time series data.  ￼
	•	Machine Learning Models: Tree-based models such as LightGBM are employed to handle complex relationships in the data.  ￼
	•	Deep Learning Models: Advanced models like DeepAR and Temporal Fusion Transformer from the GluonTS library are included for their capability to model intricate temporal dynamics.  

1. WeightedEnsemble
	•	Description: Combines the predictions of multiple models to produce a single, final prediction. It uses weighted averages of base model outputs, where the weights are optimized to minimize validation loss.
	•	Role: Improves overall performance by leveraging the strengths of multiple individual models and reducing overfitting.

2. DeepAR
	•	Description: A probabilistic forecasting model that uses Recurrent Neural Networks (RNNs) to predict future time series values. It learns patterns in sequential data and outputs a distribution for future predictions.
	•	Role: Suitable for handling time series data with dependencies over time.

3. RecursiveTabular
	•	Description: A specialized tabular model for time series tasks. It uses tabular predictors in a recursive manner, where the predictions for prior time steps are fed as inputs to future time steps.
	•	Role: Offers an efficient way to model time series data using tabular structures without deep learning.

4. PatchTST
	•	Description: A deep learning-based model specifically designed for time series forecasting. It operates on temporal patches of data and applies transformers to learn temporal patterns.
	•	Role: Capable of capturing long-term dependencies and complex temporal relationships in time series data.

5. TiDE
	•	Description: A model that combines traditional statistical methods and deep learning for time series forecasting. TiDE (Time-series Deep Ensemble) is an ensemble of deep learning models and lightweight predictors.
	•	Role: Bridges the gap between statistical models and deep learning, offering robust time series predictions.

6. DirectTabular
	•	Description: Predicts future time steps directly using a tabular structure. Unlike RecursiveTabular, it does not rely on past predictions as inputs but directly models the desired output.
	•	Role: Simpler approach for tasks where direct forecasting is sufficient.

7. ChronosFineTuned and ChronosZeroShot
	•	Description: These are fine-tuned and pre-trained versions of the Chronos model, which is tailored for time series forecasting.
	•	FineTuned Version: Trained on task-specific data to improve performance.
	•	ZeroShot Version: Utilizes pre-trained weights without further task-specific tuning.
	•	Role: Reduces training time while delivering strong forecasting capabilities.

8. AutoETS
	•	Description: An implementation of Exponential Smoothing methods for time series forecasting. It automatically selects the best ETS (Error, Trend, Seasonal) model configuration based on the data.
	•	Role: Reliable for simpler time series patterns with clear trend and seasonality components.

9. NPTS (Non-Parametric Time Series)
	•	Description: A model that does not assume any specific parametric form for the data distribution. It uses non-parametric methods to make forecasts.
	•	Role: Useful for datasets where parametric models struggle to capture the underlying data distribution.

10. DynamicOptimizedTheta
	•	Description: A statistical forecasting model based on the Theta method. It is optimized dynamically to adapt to different time series patterns.
	•	Role: Effective for time series with strong trends or seasonality.

11. SeasonalNaive
	•	Description: A baseline model that assumes the value of the current period is similar to the corresponding period in the past season.
	•	Role: Provides a simple benchmark to compare other models.

By combining these models, AutoGluon leverages their individual strengths, resulting in more robust and accurate forecasts. This ensemble approach allows the framework to adapt to various types of time series data without requiring extensive manual tuning

How These Models Fit in AutoGluon
	•	AutoGluon Tabular Time Series (AutoGluon-TS) automatically trains multiple models like the ones listed above. It evaluates their performance using validation data and selects the best-performing models for ensemble creation.
	•	The framework optimizes hyperparameters and handles feature engineering, making the models robust to different datasets.
	•	These models address a range of forecasting tasks, from simple baseline predictions (SeasonalNaive) to advanced deep learning approaches (PatchTST, TiDE).


A probabilistic forecast predicts a distribution of possible future outcomes rather than a single deterministic value. This approach accounts for uncertainty in the predictions by estimating the likelihood of different outcomes. In the context of time series forecasting, it is especially useful for quantifying risks and making decisions based on probabilistic scenarios.

WQL (Weighted Quantile Loss) is a loss function specifically designed for evaluating probabilistic forecasts. It is used in models like DeepAR, which output predictive quantiles rather than point predictions. Below is a detailed explanation of probabilistic forecasts using WQL:

1. Probabilistic Forecast:
	•	Outputs a range of possible future values with their associated probabilities. For example, instead of predicting that sales will be exactly 100 units, it might predict:
	•	90 units with 25% probability
	•	100 units with 50% probability
	•	120 units with 25% probability
	•	These probabilities are expressed as quantiles, such as the 10th percentile (lower bound), 50th percentile (median), or 90th percentile (upper bound).
	2.	Quantiles:
	•	A quantile represents a cutoff point in a probability distribution. For instance, the 0.1 quantile (10th percentile) indicates that 10% of the predicted outcomes are below this value.

3. Weighted Quantile Loss (WQL):
	•	A loss function that measures the accuracy of probabilistic forecasts. It evaluates the predicted quantiles relative to the observed values in the data.
	•	WQL assigns different weights to quantiles, prioritizing certain quantile predictions over others depending on the application.

Formula for Quantile Loss (QL)

For a given quantile  q \in (0, 1) , the quantile loss is defined as:


\text{QL}_q(y, \hat{y}) =
\begin{cases}
q \cdot (y - \hat{y}) & \text{if } y \geq \hat{y} \\
(1 - q) \cdot (\hat{y} - y) & \text{if } y < \hat{y}
\end{cases}


Where:
	•	 y  is the true observed value
	•	 \hat{y}  is the predicted quantile value

This penalizes over-predictions and under-predictions asymmetrically, depending on the quantile.

Weighted Quantile Loss (WQL)

The WQL generalizes QL by assigning a weight  w_q  to each quantile  q :


\text{WQL} = \sum_{q \in Q} w_q \cdot \text{QL}_q(y, \hat{y}_q)


Where:
	•	 Q  is the set of quantiles being predicted (e.g.,  Q = \{0.1, 0.5, 0.9\} )
	•	 w_q  is the weight assigned to quantile  q 
	•	 \hat{y}_q  is the predicted value for quantile  q 

By adjusting  w_q , the loss can emphasize certain quantiles (e.g., the median) or balance across all quantiles.

Applications of WQL in Probabilistic Forecasting
	1.	Uncertainty Quantification:
	•	Probabilistic forecasts show not just what is most likely to happen but also the range of potential outcomes and their likelihoods.
	•	WQL ensures the forecast is accurate across all quantiles.
	2.	Decision-Making Under Uncertainty:
	•	Probabilistic predictions enable businesses to prepare for best-case, worst-case, and likely scenarios.
	•	For example, in supply chain management, the 90th percentile forecast might be used to avoid stockouts.
	3.	Model Evaluation:
	•	WQL is a metric for comparing the performance of models that generate probabilistic forecasts. A lower WQL indicates better alignment between predicted quantiles and observed values.

